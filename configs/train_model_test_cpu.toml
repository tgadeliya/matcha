vocab_size = 50257
max_steps = 200000
val_steps = 256
val_every_steps = 1000
batch_size = 64
context_length = 4096
d_model = 768
device = "cpu"
num_layers = 12
num_heads = 12
d_ff = 3072
theta = 10000
lr = 0.0003
weight_decay = 0.1
betas = [0.9, 0.95]
eps = 1e-8
checkpoint_dir_path = "checkpoints/test_run"
train_data_path = "data/corpus_tokenized/tiny_stories_validation.npy"
val_data_path = "data/corpus_tokenized/tiny_stories_validation.npy"
